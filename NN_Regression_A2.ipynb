{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isabella-as/Intelligent-Systems-Assignments/blob/main/NN_Regression_A2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Regression Problem**\n",
        "**Neural Network Approach (Task 1)**"
      ],
      "metadata": {
        "id": "nS_ZWMtw0Oyt"
      },
      "id": "nS_ZWMtw0Oyt"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c6428f9d",
      "metadata": {
        "id": "c6428f9d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error,accuracy_score,classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below loads the dataset, separates the features \"x\" from the target \"y\" and converts them from pandas objects into NumPy arrays using \".values\", and prints the shape of the feature matrix to show how many samples and features are available."
      ],
      "metadata": {
        "id": "kh2fTwZB9o6_"
      },
      "id": "kh2fTwZB9o6_"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0a386c10",
      "metadata": {
        "id": "0a386c10",
        "outputId": "fa0e2c9a-51bc-4a06-aee8-d7a4f4b67f42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(442, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Regression dataset\n",
        "\n",
        "diabetes= datasets.load_diabetes(as_frame=True)\n",
        "\n",
        "X = diabetes.data.values\n",
        "y = diabetes.target.values\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, the dataset is split into training and testing sets using an 80/20 ratio, where 80% of the data (Xtr, ytr) is used to train the model and 20% (Xte, yte) is reserved for testing. The parameter random_state=42 ensures that the split is reproducible, so the same division of data will be obtained each time the code is ran."
      ],
      "metadata": {
        "id": "FB9AMpkwMnDZ"
      },
      "id": "FB9AMpkwMnDZ"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "aafa6216",
      "metadata": {
        "id": "aafa6216"
      },
      "outputs": [],
      "source": [
        "#train test spliting\n",
        "test_size=0.2\n",
        "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=test_size, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The input features are standardized using StandardScaler. They are rescaled so that each feature has a mean of 0 and a standard deviation of 1. This prevents features with larger numeric ranges from dominating the learning process. The scaler is fit on the training set and then applied to both the training and testing sets to avoid data leakage."
      ],
      "metadata": {
        "id": "5B44HV8CNNK1"
      },
      "id": "5B44HV8CNNK1"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2dfc9566",
      "metadata": {
        "id": "2dfc9566"
      },
      "outputs": [],
      "source": [
        "# Standardize features\n",
        "scaler=StandardScaler()\n",
        "Xtr= scaler.fit_transform(Xtr)\n",
        "Xte= scaler.transform(Xte)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a multilayer perceptron (MLP) model for regression using PyTorch. The network takes the input features and passes them through four fully connected hidden layers, each with 64 neurons and ReLU activation functions for non-linearity. To reduce overfitting, dropout is applied after each hidden layer, randomly setting 50% of the neurons to zero during training. Finally, the output layer (self.out) produces a single value (output_size=1), which corresponds to the continuous regression target. The forward method specifies the sequence of operations that the data follows as it moves through the network."
      ],
      "metadata": {
        "id": "bLpHpJVZPQL2"
      },
      "id": "bLpHpJVZPQL2"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "99c0966d",
      "metadata": {
        "id": "99c0966d"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, output_size=1, dropout_prob=0.5):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, 64)\n",
        "        self.fc4 = nn.Linear(64, 64)\n",
        "        self.out = nn.Linear(64, output_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.out(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following code the main training hyperparameters are defined. The model is trained for 100 epochs- the entire training set will be passed through the network 100 times. The learning rate is set to 0.0005 (controls the step size during optimization). A dropout rate of 0.1 implies that 10% of the neurons are randomly deactivated during training to reduce overfitting. Finally, the batch size is set to 64, so the model processes 64 samples at a time before updating the weights. These parameters affect how fast the network learns and how well it generalizes."
      ],
      "metadata": {
        "id": "BtlFV8eHRJix"
      },
      "id": "BtlFV8eHRJix"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2ab972de",
      "metadata": {
        "id": "2ab972de"
      },
      "outputs": [],
      "source": [
        "num_epochs=500\n",
        "lr=0.0005\n",
        "dropout=0.1\n",
        "batch_size=64"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this stage, the training and testing sets are converted from NumPy arrays into PyTorch tensors so they can be processed by the neural network (type float 32). The training inputs (Xtr) and outputs (ytr) are combined into a TensorDataset, which stores them as pairs. This dataset is then wrapped in a DataLoader, which automatically handles splitting the data into mini-batches (here of size 64, as previously defined) and shuffles the order of the samples at each epoch, helping the model generalize better by preventing it from seeing the data in the same order every time."
      ],
      "metadata": {
        "id": "Lf3cbujZR035"
      },
      "id": "Lf3cbujZR035"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "49298ada",
      "metadata": {
        "id": "49298ada"
      },
      "outputs": [],
      "source": [
        "Xtr = torch.tensor(Xtr, dtype=torch.float32)\n",
        "ytr = torch.tensor(ytr, dtype=torch.float32)\n",
        "Xte = torch.tensor(Xte, dtype=torch.float32)\n",
        "yte = torch.tensor(yte, dtype=torch.float32)\n",
        "\n",
        "# Wrap Xtr and ytr into a dataset\n",
        "train_dataset = TensorDataset(Xtr, ytr)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we set up the model, loss function, and optimizer. The code first checks whether a GPU is available and assigns the device accordingly. The MLP model is then created with an input size matching the number of features and the specified dropout probability, and it is moved to the chosen device. For the regression problem the mean square erro loss is used (MSELoss()) . Finally, the Adam optimizer is defined with the chosen learning rate (lr), which controls de rate in which the modelâ€™s parameters are updated during training."
      ],
      "metadata": {
        "id": "4Yl0FqhMTsrE"
      },
      "id": "4Yl0FqhMTsrE"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8e649c6f",
      "metadata": {
        "id": "8e649c6f"
      },
      "outputs": [],
      "source": [
        "# Model, Loss, Optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = MLP(input_size=Xtr.shape[1], dropout_prob=dropout).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block implements the training loop for the neural network. For each epoch, the model is set to training mode and the data is processed in small batches from the DataLoader. Each batch of features and labels is moved to the selected device (CPU or GPU), goes through the model to produce predictions (logits), and compared to the true targets using the chosen loss function (for regression- MSELoss). Before backpropagation, the optimizerâ€™s gradients are reset with zero_grad(). The backward() call computes gradients of the loss with respect to the model parameters, and optimizer.step() updates those parameters. The average loss of each epoch (from each epoch's batches) is printed to monitor the training progress."
      ],
      "metadata": {
        "id": "MghwQg5TlORR"
      },
      "id": "MghwQg5TlORR"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e02dec66",
      "metadata": {
        "id": "e02dec66",
        "outputId": "16b2e8bf-9f85-42bc-954a-7f6c9e56a998",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/500], Loss: 30363.6514\n",
            "Epoch [2/500], Loss: 29195.9313\n",
            "Epoch [3/500], Loss: 29540.0446\n",
            "Epoch [4/500], Loss: 30064.0345\n",
            "Epoch [5/500], Loss: 29829.4313\n",
            "Epoch [6/500], Loss: 29216.3893\n",
            "Epoch [7/500], Loss: 29443.5049\n",
            "Epoch [8/500], Loss: 28794.3076\n",
            "Epoch [9/500], Loss: 28392.2956\n",
            "Epoch [10/500], Loss: 27669.9049\n",
            "Epoch [11/500], Loss: 26974.6585\n",
            "Epoch [12/500], Loss: 24884.3473\n",
            "Epoch [13/500], Loss: 21963.1986\n",
            "Epoch [14/500], Loss: 18957.8797\n",
            "Epoch [15/500], Loss: 15809.9943\n",
            "Epoch [16/500], Loss: 11974.2046\n",
            "Epoch [17/500], Loss: 8228.3551\n",
            "Epoch [18/500], Loss: 6856.1934\n",
            "Epoch [19/500], Loss: 5956.9561\n",
            "Epoch [20/500], Loss: 5764.4419\n",
            "Epoch [21/500], Loss: 5399.9675\n",
            "Epoch [22/500], Loss: 4885.1201\n",
            "Epoch [23/500], Loss: 4599.4541\n",
            "Epoch [24/500], Loss: 4568.4321\n",
            "Epoch [25/500], Loss: 4834.3208\n",
            "Epoch [26/500], Loss: 4534.5582\n",
            "Epoch [27/500], Loss: 4437.9703\n",
            "Epoch [28/500], Loss: 4380.2599\n",
            "Epoch [29/500], Loss: 4214.3436\n",
            "Epoch [30/500], Loss: 4236.8692\n",
            "Epoch [31/500], Loss: 4106.7338\n",
            "Epoch [32/500], Loss: 4071.9810\n",
            "Epoch [33/500], Loss: 3905.5055\n",
            "Epoch [34/500], Loss: 3985.2415\n",
            "Epoch [35/500], Loss: 4120.0251\n",
            "Epoch [36/500], Loss: 3720.2073\n",
            "Epoch [37/500], Loss: 3840.9517\n",
            "Epoch [38/500], Loss: 3757.5124\n",
            "Epoch [39/500], Loss: 3547.4089\n",
            "Epoch [40/500], Loss: 3556.4150\n",
            "Epoch [41/500], Loss: 3530.1991\n",
            "Epoch [42/500], Loss: 3543.0558\n",
            "Epoch [43/500], Loss: 3386.0989\n",
            "Epoch [44/500], Loss: 3731.0763\n",
            "Epoch [45/500], Loss: 3718.4585\n",
            "Epoch [46/500], Loss: 3593.8069\n",
            "Epoch [47/500], Loss: 3688.0852\n",
            "Epoch [48/500], Loss: 3330.5131\n",
            "Epoch [49/500], Loss: 3571.7417\n",
            "Epoch [50/500], Loss: 3790.2958\n",
            "Epoch [51/500], Loss: 3507.8111\n",
            "Epoch [52/500], Loss: 3498.3181\n",
            "Epoch [53/500], Loss: 3281.4045\n",
            "Epoch [54/500], Loss: 3427.4990\n",
            "Epoch [55/500], Loss: 3292.6696\n",
            "Epoch [56/500], Loss: 3558.5404\n",
            "Epoch [57/500], Loss: 3225.9572\n",
            "Epoch [58/500], Loss: 3597.9776\n",
            "Epoch [59/500], Loss: 3362.4323\n",
            "Epoch [60/500], Loss: 3451.6736\n",
            "Epoch [61/500], Loss: 3198.2484\n",
            "Epoch [62/500], Loss: 3199.1732\n",
            "Epoch [63/500], Loss: 3193.7004\n",
            "Epoch [64/500], Loss: 3014.7751\n",
            "Epoch [65/500], Loss: 3177.8595\n",
            "Epoch [66/500], Loss: 3135.3525\n",
            "Epoch [67/500], Loss: 3033.8442\n",
            "Epoch [68/500], Loss: 3474.2097\n",
            "Epoch [69/500], Loss: 3048.4176\n",
            "Epoch [70/500], Loss: 3266.6971\n",
            "Epoch [71/500], Loss: 3198.2623\n",
            "Epoch [72/500], Loss: 3236.4130\n",
            "Epoch [73/500], Loss: 3363.3317\n",
            "Epoch [74/500], Loss: 3353.6274\n",
            "Epoch [75/500], Loss: 3108.7158\n",
            "Epoch [76/500], Loss: 3234.7583\n",
            "Epoch [77/500], Loss: 3178.2650\n",
            "Epoch [78/500], Loss: 3059.7295\n",
            "Epoch [79/500], Loss: 3142.8955\n",
            "Epoch [80/500], Loss: 3331.3556\n",
            "Epoch [81/500], Loss: 3105.8657\n",
            "Epoch [82/500], Loss: 3087.5234\n",
            "Epoch [83/500], Loss: 3333.8872\n",
            "Epoch [84/500], Loss: 3192.1253\n",
            "Epoch [85/500], Loss: 3362.4621\n",
            "Epoch [86/500], Loss: 3095.1109\n",
            "Epoch [87/500], Loss: 3207.7250\n",
            "Epoch [88/500], Loss: 3285.7122\n",
            "Epoch [89/500], Loss: 3136.9619\n",
            "Epoch [90/500], Loss: 2909.2915\n",
            "Epoch [91/500], Loss: 3042.7334\n",
            "Epoch [92/500], Loss: 2791.1872\n",
            "Epoch [93/500], Loss: 3031.5594\n",
            "Epoch [94/500], Loss: 2949.7828\n",
            "Epoch [95/500], Loss: 3091.2598\n",
            "Epoch [96/500], Loss: 3020.1867\n",
            "Epoch [97/500], Loss: 3226.9964\n",
            "Epoch [98/500], Loss: 2993.3176\n",
            "Epoch [99/500], Loss: 2981.8879\n",
            "Epoch [100/500], Loss: 3203.9147\n",
            "Epoch [101/500], Loss: 3106.5495\n",
            "Epoch [102/500], Loss: 2956.4934\n",
            "Epoch [103/500], Loss: 2948.7113\n",
            "Epoch [104/500], Loss: 3160.5779\n",
            "Epoch [105/500], Loss: 2833.9267\n",
            "Epoch [106/500], Loss: 3026.4751\n",
            "Epoch [107/500], Loss: 3036.7899\n",
            "Epoch [108/500], Loss: 2880.9146\n",
            "Epoch [109/500], Loss: 2911.5159\n",
            "Epoch [110/500], Loss: 3085.6930\n",
            "Epoch [111/500], Loss: 3169.6394\n",
            "Epoch [112/500], Loss: 3000.0411\n",
            "Epoch [113/500], Loss: 2914.5508\n",
            "Epoch [114/500], Loss: 2802.9670\n",
            "Epoch [115/500], Loss: 3142.5858\n",
            "Epoch [116/500], Loss: 2972.9504\n",
            "Epoch [117/500], Loss: 2854.9921\n",
            "Epoch [118/500], Loss: 3216.0600\n",
            "Epoch [119/500], Loss: 3266.8868\n",
            "Epoch [120/500], Loss: 2996.5568\n",
            "Epoch [121/500], Loss: 3051.7438\n",
            "Epoch [122/500], Loss: 2984.3770\n",
            "Epoch [123/500], Loss: 2921.8340\n",
            "Epoch [124/500], Loss: 3124.2101\n",
            "Epoch [125/500], Loss: 2827.7837\n",
            "Epoch [126/500], Loss: 2956.3517\n",
            "Epoch [127/500], Loss: 2875.3977\n",
            "Epoch [128/500], Loss: 3161.3418\n",
            "Epoch [129/500], Loss: 2897.3775\n",
            "Epoch [130/500], Loss: 2955.1923\n",
            "Epoch [131/500], Loss: 3014.6699\n",
            "Epoch [132/500], Loss: 2917.3813\n",
            "Epoch [133/500], Loss: 3110.5832\n",
            "Epoch [134/500], Loss: 2914.2345\n",
            "Epoch [135/500], Loss: 2996.4956\n",
            "Epoch [136/500], Loss: 2952.6398\n",
            "Epoch [137/500], Loss: 2887.5872\n",
            "Epoch [138/500], Loss: 2977.9931\n",
            "Epoch [139/500], Loss: 2927.4646\n",
            "Epoch [140/500], Loss: 2915.2842\n",
            "Epoch [141/500], Loss: 2895.3450\n",
            "Epoch [142/500], Loss: 2816.7232\n",
            "Epoch [143/500], Loss: 2940.4930\n",
            "Epoch [144/500], Loss: 2981.6965\n",
            "Epoch [145/500], Loss: 2881.0015\n",
            "Epoch [146/500], Loss: 3054.5902\n",
            "Epoch [147/500], Loss: 2962.2649\n",
            "Epoch [148/500], Loss: 2860.2758\n",
            "Epoch [149/500], Loss: 3038.4433\n",
            "Epoch [150/500], Loss: 2874.5841\n",
            "Epoch [151/500], Loss: 3106.9132\n",
            "Epoch [152/500], Loss: 2923.7367\n",
            "Epoch [153/500], Loss: 3027.0750\n",
            "Epoch [154/500], Loss: 2801.6343\n",
            "Epoch [155/500], Loss: 2687.1572\n",
            "Epoch [156/500], Loss: 2953.6007\n",
            "Epoch [157/500], Loss: 2846.7896\n",
            "Epoch [158/500], Loss: 2691.7001\n",
            "Epoch [159/500], Loss: 2884.5403\n",
            "Epoch [160/500], Loss: 2791.8385\n",
            "Epoch [161/500], Loss: 2657.3581\n",
            "Epoch [162/500], Loss: 2879.3365\n",
            "Epoch [163/500], Loss: 2816.4170\n",
            "Epoch [164/500], Loss: 3114.7962\n",
            "Epoch [165/500], Loss: 3035.9151\n",
            "Epoch [166/500], Loss: 2814.4460\n",
            "Epoch [167/500], Loss: 2870.3960\n",
            "Epoch [168/500], Loss: 2984.2297\n",
            "Epoch [169/500], Loss: 2936.3321\n",
            "Epoch [170/500], Loss: 2617.5134\n",
            "Epoch [171/500], Loss: 2679.2192\n",
            "Epoch [172/500], Loss: 2602.2038\n",
            "Epoch [173/500], Loss: 2834.8680\n",
            "Epoch [174/500], Loss: 2955.5499\n",
            "Epoch [175/500], Loss: 2615.5858\n",
            "Epoch [176/500], Loss: 2807.9237\n",
            "Epoch [177/500], Loss: 2752.4897\n",
            "Epoch [178/500], Loss: 2904.9362\n",
            "Epoch [179/500], Loss: 2789.5728\n",
            "Epoch [180/500], Loss: 2916.9257\n",
            "Epoch [181/500], Loss: 2722.7884\n",
            "Epoch [182/500], Loss: 2801.2844\n",
            "Epoch [183/500], Loss: 2733.9913\n",
            "Epoch [184/500], Loss: 2896.2131\n",
            "Epoch [185/500], Loss: 2846.9610\n",
            "Epoch [186/500], Loss: 3108.6982\n",
            "Epoch [187/500], Loss: 2738.1865\n",
            "Epoch [188/500], Loss: 2507.3407\n",
            "Epoch [189/500], Loss: 2870.4347\n",
            "Epoch [190/500], Loss: 2740.1051\n",
            "Epoch [191/500], Loss: 2774.4124\n",
            "Epoch [192/500], Loss: 2884.6561\n",
            "Epoch [193/500], Loss: 2962.5723\n",
            "Epoch [194/500], Loss: 2675.6417\n",
            "Epoch [195/500], Loss: 2634.3459\n",
            "Epoch [196/500], Loss: 2632.2533\n",
            "Epoch [197/500], Loss: 2825.7079\n",
            "Epoch [198/500], Loss: 2640.4514\n",
            "Epoch [199/500], Loss: 2740.6637\n",
            "Epoch [200/500], Loss: 2670.9866\n",
            "Epoch [201/500], Loss: 2486.2892\n",
            "Epoch [202/500], Loss: 2600.8609\n",
            "Epoch [203/500], Loss: 2719.8527\n",
            "Epoch [204/500], Loss: 2942.2311\n",
            "Epoch [205/500], Loss: 2663.9180\n",
            "Epoch [206/500], Loss: 2804.9717\n",
            "Epoch [207/500], Loss: 2934.3670\n",
            "Epoch [208/500], Loss: 2741.0113\n",
            "Epoch [209/500], Loss: 2627.1927\n",
            "Epoch [210/500], Loss: 2865.8352\n",
            "Epoch [211/500], Loss: 2644.5176\n",
            "Epoch [212/500], Loss: 2851.6820\n",
            "Epoch [213/500], Loss: 2684.4515\n",
            "Epoch [214/500], Loss: 2729.3701\n",
            "Epoch [215/500], Loss: 2822.9873\n",
            "Epoch [216/500], Loss: 2969.9225\n",
            "Epoch [217/500], Loss: 2746.1505\n",
            "Epoch [218/500], Loss: 2735.7485\n",
            "Epoch [219/500], Loss: 2732.7505\n",
            "Epoch [220/500], Loss: 2753.7852\n",
            "Epoch [221/500], Loss: 2667.9478\n",
            "Epoch [222/500], Loss: 2661.7903\n",
            "Epoch [223/500], Loss: 2691.7359\n",
            "Epoch [224/500], Loss: 2782.5635\n",
            "Epoch [225/500], Loss: 2648.5652\n",
            "Epoch [226/500], Loss: 2877.5750\n",
            "Epoch [227/500], Loss: 2511.9410\n",
            "Epoch [228/500], Loss: 2815.9530\n",
            "Epoch [229/500], Loss: 2875.5137\n",
            "Epoch [230/500], Loss: 2669.2930\n",
            "Epoch [231/500], Loss: 2677.4052\n",
            "Epoch [232/500], Loss: 2622.0509\n",
            "Epoch [233/500], Loss: 2857.4404\n",
            "Epoch [234/500], Loss: 2717.8113\n",
            "Epoch [235/500], Loss: 2899.3362\n",
            "Epoch [236/500], Loss: 2823.8167\n",
            "Epoch [237/500], Loss: 2826.2067\n",
            "Epoch [238/500], Loss: 2829.0149\n",
            "Epoch [239/500], Loss: 2589.4388\n",
            "Epoch [240/500], Loss: 2485.1580\n",
            "Epoch [241/500], Loss: 2683.7159\n",
            "Epoch [242/500], Loss: 2675.5412\n",
            "Epoch [243/500], Loss: 2515.9951\n",
            "Epoch [244/500], Loss: 2694.3781\n",
            "Epoch [245/500], Loss: 2396.7206\n",
            "Epoch [246/500], Loss: 2493.8139\n",
            "Epoch [247/500], Loss: 2803.3055\n",
            "Epoch [248/500], Loss: 2511.0896\n",
            "Epoch [249/500], Loss: 2736.0499\n",
            "Epoch [250/500], Loss: 2640.0816\n",
            "Epoch [251/500], Loss: 2675.6556\n",
            "Epoch [252/500], Loss: 2856.6662\n",
            "Epoch [253/500], Loss: 2571.2374\n",
            "Epoch [254/500], Loss: 2511.6575\n",
            "Epoch [255/500], Loss: 2582.5548\n",
            "Epoch [256/500], Loss: 2668.5283\n",
            "Epoch [257/500], Loss: 2729.3981\n",
            "Epoch [258/500], Loss: 2543.5788\n",
            "Epoch [259/500], Loss: 2500.6309\n",
            "Epoch [260/500], Loss: 2767.7243\n",
            "Epoch [261/500], Loss: 2667.6802\n",
            "Epoch [262/500], Loss: 2550.8844\n",
            "Epoch [263/500], Loss: 2673.1146\n",
            "Epoch [264/500], Loss: 2469.9215\n",
            "Epoch [265/500], Loss: 2479.1631\n",
            "Epoch [266/500], Loss: 2694.8211\n",
            "Epoch [267/500], Loss: 2572.9266\n",
            "Epoch [268/500], Loss: 2680.5457\n",
            "Epoch [269/500], Loss: 2667.5127\n",
            "Epoch [270/500], Loss: 2452.1080\n",
            "Epoch [271/500], Loss: 2593.9163\n",
            "Epoch [272/500], Loss: 2681.4444\n",
            "Epoch [273/500], Loss: 2649.9038\n",
            "Epoch [274/500], Loss: 2621.3634\n",
            "Epoch [275/500], Loss: 2615.4677\n",
            "Epoch [276/500], Loss: 2464.4824\n",
            "Epoch [277/500], Loss: 2318.4740\n",
            "Epoch [278/500], Loss: 2565.2061\n",
            "Epoch [279/500], Loss: 2627.1137\n",
            "Epoch [280/500], Loss: 2658.8414\n",
            "Epoch [281/500], Loss: 2697.4157\n",
            "Epoch [282/500], Loss: 2518.7285\n",
            "Epoch [283/500], Loss: 2462.8397\n",
            "Epoch [284/500], Loss: 2543.7266\n",
            "Epoch [285/500], Loss: 2605.4095\n",
            "Epoch [286/500], Loss: 2557.5268\n",
            "Epoch [287/500], Loss: 2574.3204\n",
            "Epoch [288/500], Loss: 2577.4776\n",
            "Epoch [289/500], Loss: 2453.3268\n",
            "Epoch [290/500], Loss: 2615.3161\n",
            "Epoch [291/500], Loss: 2624.1316\n",
            "Epoch [292/500], Loss: 2364.2213\n",
            "Epoch [293/500], Loss: 2616.3155\n",
            "Epoch [294/500], Loss: 2605.1836\n",
            "Epoch [295/500], Loss: 2641.2809\n",
            "Epoch [296/500], Loss: 2471.1118\n",
            "Epoch [297/500], Loss: 2614.0715\n",
            "Epoch [298/500], Loss: 2591.3443\n",
            "Epoch [299/500], Loss: 2440.1520\n",
            "Epoch [300/500], Loss: 2466.6184\n",
            "Epoch [301/500], Loss: 2455.0163\n",
            "Epoch [302/500], Loss: 2628.4682\n",
            "Epoch [303/500], Loss: 2842.6949\n",
            "Epoch [304/500], Loss: 2488.9350\n",
            "Epoch [305/500], Loss: 2612.4024\n",
            "Epoch [306/500], Loss: 2532.6481\n",
            "Epoch [307/500], Loss: 2525.2934\n",
            "Epoch [308/500], Loss: 2531.9895\n",
            "Epoch [309/500], Loss: 2639.5693\n",
            "Epoch [310/500], Loss: 2423.0200\n",
            "Epoch [311/500], Loss: 2491.7010\n",
            "Epoch [312/500], Loss: 2590.4842\n",
            "Epoch [313/500], Loss: 2403.5277\n",
            "Epoch [314/500], Loss: 2615.1153\n",
            "Epoch [315/500], Loss: 2494.6516\n",
            "Epoch [316/500], Loss: 2519.0235\n",
            "Epoch [317/500], Loss: 2462.0161\n",
            "Epoch [318/500], Loss: 2548.2469\n",
            "Epoch [319/500], Loss: 2754.1437\n",
            "Epoch [320/500], Loss: 2405.1314\n",
            "Epoch [321/500], Loss: 2330.5261\n",
            "Epoch [322/500], Loss: 2468.3040\n",
            "Epoch [323/500], Loss: 2499.0745\n",
            "Epoch [324/500], Loss: 2303.5386\n",
            "Epoch [325/500], Loss: 2484.8664\n",
            "Epoch [326/500], Loss: 2447.8652\n",
            "Epoch [327/500], Loss: 2471.6896\n",
            "Epoch [328/500], Loss: 2607.3177\n",
            "Epoch [329/500], Loss: 2385.1935\n",
            "Epoch [330/500], Loss: 2562.4475\n",
            "Epoch [331/500], Loss: 2641.8135\n",
            "Epoch [332/500], Loss: 2407.5655\n",
            "Epoch [333/500], Loss: 2596.4451\n",
            "Epoch [334/500], Loss: 2512.4539\n",
            "Epoch [335/500], Loss: 2344.6549\n",
            "Epoch [336/500], Loss: 2294.4510\n",
            "Epoch [337/500], Loss: 2342.7462\n",
            "Epoch [338/500], Loss: 2428.9799\n",
            "Epoch [339/500], Loss: 2469.8771\n",
            "Epoch [340/500], Loss: 2435.2954\n",
            "Epoch [341/500], Loss: 2390.2189\n",
            "Epoch [342/500], Loss: 2352.9573\n",
            "Epoch [343/500], Loss: 2513.6176\n",
            "Epoch [344/500], Loss: 2531.9739\n",
            "Epoch [345/500], Loss: 2581.1088\n",
            "Epoch [346/500], Loss: 2357.2365\n",
            "Epoch [347/500], Loss: 2315.9588\n",
            "Epoch [348/500], Loss: 2409.8174\n",
            "Epoch [349/500], Loss: 2408.3600\n",
            "Epoch [350/500], Loss: 2331.2787\n",
            "Epoch [351/500], Loss: 2263.3457\n",
            "Epoch [352/500], Loss: 2502.9843\n",
            "Epoch [353/500], Loss: 2800.9056\n",
            "Epoch [354/500], Loss: 2348.1972\n",
            "Epoch [355/500], Loss: 2455.2474\n",
            "Epoch [356/500], Loss: 2597.2351\n",
            "Epoch [357/500], Loss: 2271.2453\n",
            "Epoch [358/500], Loss: 2225.7420\n",
            "Epoch [359/500], Loss: 2383.3359\n",
            "Epoch [360/500], Loss: 2532.2383\n",
            "Epoch [361/500], Loss: 2377.6382\n",
            "Epoch [362/500], Loss: 2384.0396\n",
            "Epoch [363/500], Loss: 2591.2521\n",
            "Epoch [364/500], Loss: 2339.6345\n",
            "Epoch [365/500], Loss: 2193.6274\n",
            "Epoch [366/500], Loss: 2330.6155\n",
            "Epoch [367/500], Loss: 2350.8303\n",
            "Epoch [368/500], Loss: 2373.3855\n",
            "Epoch [369/500], Loss: 2502.0908\n",
            "Epoch [370/500], Loss: 2263.5868\n",
            "Epoch [371/500], Loss: 2553.8162\n",
            "Epoch [372/500], Loss: 2388.4592\n",
            "Epoch [373/500], Loss: 2282.5404\n",
            "Epoch [374/500], Loss: 2516.4637\n",
            "Epoch [375/500], Loss: 2449.0102\n",
            "Epoch [376/500], Loss: 2415.0093\n",
            "Epoch [377/500], Loss: 2180.0142\n",
            "Epoch [378/500], Loss: 2410.1683\n",
            "Epoch [379/500], Loss: 2354.8487\n",
            "Epoch [380/500], Loss: 2320.2864\n",
            "Epoch [381/500], Loss: 2333.4908\n",
            "Epoch [382/500], Loss: 2306.6271\n",
            "Epoch [383/500], Loss: 2240.0152\n",
            "Epoch [384/500], Loss: 2267.4423\n",
            "Epoch [385/500], Loss: 2219.8987\n",
            "Epoch [386/500], Loss: 2554.7161\n",
            "Epoch [387/500], Loss: 2272.6962\n",
            "Epoch [388/500], Loss: 2339.3130\n",
            "Epoch [389/500], Loss: 2251.0620\n",
            "Epoch [390/500], Loss: 2332.8360\n",
            "Epoch [391/500], Loss: 2353.0254\n",
            "Epoch [392/500], Loss: 2425.8593\n",
            "Epoch [393/500], Loss: 2366.2426\n",
            "Epoch [394/500], Loss: 2365.4781\n",
            "Epoch [395/500], Loss: 2555.6059\n",
            "Epoch [396/500], Loss: 2243.3584\n",
            "Epoch [397/500], Loss: 2742.3566\n",
            "Epoch [398/500], Loss: 2321.1234\n",
            "Epoch [399/500], Loss: 2191.4201\n",
            "Epoch [400/500], Loss: 2254.1259\n",
            "Epoch [401/500], Loss: 2249.7689\n",
            "Epoch [402/500], Loss: 2122.4853\n",
            "Epoch [403/500], Loss: 2356.4440\n",
            "Epoch [404/500], Loss: 2404.4742\n",
            "Epoch [405/500], Loss: 2433.1667\n",
            "Epoch [406/500], Loss: 2330.1227\n",
            "Epoch [407/500], Loss: 2258.2516\n",
            "Epoch [408/500], Loss: 2420.2229\n",
            "Epoch [409/500], Loss: 2339.3364\n",
            "Epoch [410/500], Loss: 2241.0542\n",
            "Epoch [411/500], Loss: 2306.6425\n",
            "Epoch [412/500], Loss: 2348.5691\n",
            "Epoch [413/500], Loss: 2080.6737\n",
            "Epoch [414/500], Loss: 2347.3801\n",
            "Epoch [415/500], Loss: 2290.4559\n",
            "Epoch [416/500], Loss: 2221.3054\n",
            "Epoch [417/500], Loss: 2337.1974\n",
            "Epoch [418/500], Loss: 2179.5269\n",
            "Epoch [419/500], Loss: 2264.3221\n",
            "Epoch [420/500], Loss: 2223.9299\n",
            "Epoch [421/500], Loss: 2546.5581\n",
            "Epoch [422/500], Loss: 2467.7046\n",
            "Epoch [423/500], Loss: 2125.2128\n",
            "Epoch [424/500], Loss: 2262.6675\n",
            "Epoch [425/500], Loss: 2271.3259\n",
            "Epoch [426/500], Loss: 2203.8153\n",
            "Epoch [427/500], Loss: 2281.4882\n",
            "Epoch [428/500], Loss: 2282.8957\n",
            "Epoch [429/500], Loss: 2193.0820\n",
            "Epoch [430/500], Loss: 2222.0486\n",
            "Epoch [431/500], Loss: 2131.6324\n",
            "Epoch [432/500], Loss: 2250.7639\n",
            "Epoch [433/500], Loss: 2353.6760\n",
            "Epoch [434/500], Loss: 2190.5630\n",
            "Epoch [435/500], Loss: 2320.8201\n",
            "Epoch [436/500], Loss: 1989.4147\n",
            "Epoch [437/500], Loss: 2505.0976\n",
            "Epoch [438/500], Loss: 2245.8801\n",
            "Epoch [439/500], Loss: 2415.3919\n",
            "Epoch [440/500], Loss: 2459.6601\n",
            "Epoch [441/500], Loss: 2226.7002\n",
            "Epoch [442/500], Loss: 2277.7416\n",
            "Epoch [443/500], Loss: 2194.6660\n",
            "Epoch [444/500], Loss: 2169.8258\n",
            "Epoch [445/500], Loss: 2317.0535\n",
            "Epoch [446/500], Loss: 2222.7425\n",
            "Epoch [447/500], Loss: 2091.3031\n",
            "Epoch [448/500], Loss: 2274.1866\n",
            "Epoch [449/500], Loss: 2301.7902\n",
            "Epoch [450/500], Loss: 2255.1476\n",
            "Epoch [451/500], Loss: 2077.1950\n",
            "Epoch [452/500], Loss: 2197.0030\n",
            "Epoch [453/500], Loss: 2140.5075\n",
            "Epoch [454/500], Loss: 2284.0717\n",
            "Epoch [455/500], Loss: 2269.4762\n",
            "Epoch [456/500], Loss: 2180.8327\n",
            "Epoch [457/500], Loss: 2264.1264\n",
            "Epoch [458/500], Loss: 2371.2191\n",
            "Epoch [459/500], Loss: 2204.9502\n",
            "Epoch [460/500], Loss: 2200.5790\n",
            "Epoch [461/500], Loss: 2221.4677\n",
            "Epoch [462/500], Loss: 2248.8821\n",
            "Epoch [463/500], Loss: 2358.3933\n",
            "Epoch [464/500], Loss: 2321.7156\n",
            "Epoch [465/500], Loss: 2284.6117\n",
            "Epoch [466/500], Loss: 2273.7189\n",
            "Epoch [467/500], Loss: 2184.7395\n",
            "Epoch [468/500], Loss: 2016.5320\n",
            "Epoch [469/500], Loss: 2197.3461\n",
            "Epoch [470/500], Loss: 2216.9797\n",
            "Epoch [471/500], Loss: 2359.3373\n",
            "Epoch [472/500], Loss: 2173.1596\n",
            "Epoch [473/500], Loss: 2175.0776\n",
            "Epoch [474/500], Loss: 2165.7874\n",
            "Epoch [475/500], Loss: 2205.7524\n",
            "Epoch [476/500], Loss: 2102.1050\n",
            "Epoch [477/500], Loss: 2228.3725\n",
            "Epoch [478/500], Loss: 2314.5896\n",
            "Epoch [479/500], Loss: 2215.6021\n",
            "Epoch [480/500], Loss: 2010.4431\n",
            "Epoch [481/500], Loss: 2219.7375\n",
            "Epoch [482/500], Loss: 2186.4588\n",
            "Epoch [483/500], Loss: 2094.8961\n",
            "Epoch [484/500], Loss: 2320.2154\n",
            "Epoch [485/500], Loss: 2029.7850\n",
            "Epoch [486/500], Loss: 2151.0020\n",
            "Epoch [487/500], Loss: 2467.6726\n",
            "Epoch [488/500], Loss: 2012.8084\n",
            "Epoch [489/500], Loss: 2166.0869\n",
            "Epoch [490/500], Loss: 2076.7876\n",
            "Epoch [491/500], Loss: 2347.7488\n",
            "Epoch [492/500], Loss: 1925.2229\n",
            "Epoch [493/500], Loss: 2194.1570\n",
            "Epoch [494/500], Loss: 2325.9242\n",
            "Epoch [495/500], Loss: 2379.5659\n",
            "Epoch [496/500], Loss: 2080.8772\n",
            "Epoch [497/500], Loss: 2228.8572\n",
            "Epoch [498/500], Loss: 1921.3061\n",
            "Epoch [499/500], Loss: 2098.0399\n",
            "Epoch [500/500], Loss: 2181.0259\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for batch_x, batch_y in train_dataloader:\n",
        "        batch_x = batch_x.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "\n",
        "        logits = model(batch_x)\n",
        "        loss = criterion(logits, batch_y.view(-1, 1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_dataloader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To finalize, the model is used to make predictions on the test data (Xte), and these predictions (y_pred) are compared with the true target values (yte) to evaluate performance. For regression problems, the Mean Squared Error (MSE) is used to measures how far off the predicted values are from the actual values on average. The lower the values the better the performance (better fit to unseen data)."
      ],
      "metadata": {
        "id": "mIyK-7Yxr7kL"
      },
      "id": "mIyK-7Yxr7kL"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "dc271f10",
      "metadata": {
        "id": "dc271f10",
        "outputId": "841d56ff-da8a-46c8-cacd-96d67069aefe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE:2939.66357421875\n"
          ]
        }
      ],
      "source": [
        "y_pred=model(Xte)\n",
        "print(f'MSE:{mean_squared_error(yte.detach().numpy(),y_pred.detach().numpy())}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "SI2526",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}